---
source: lex-parser/tests/tokenizer_elements.rs
expression: tokens
---
[
    (
        Number(
            "1",
        ),
        0..1,
    ),
    (
        Period,
        1..2,
    ),
    (
        Whitespace(
            1,
        ),
        2..3,
    ),
    (
        Text(
            "Introduction",
        ),
        3..15,
    ),
    (
        Colon,
        15..16,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        16..17,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        17..18,
    ),
    (
        Indent(
            [
                (
                    Indentation,
                    18..22,
                ),
            ],
        ),
        0..0,
    ),
    (
        Text(
            "This",
        ),
        22..26,
    ),
    (
        Whitespace(
            1,
        ),
        26..27,
    ),
    (
        Text(
            "session",
        ),
        27..34,
    ),
    (
        Whitespace(
            1,
        ),
        34..35,
    ),
    (
        Text(
            "has",
        ),
        35..38,
    ),
    (
        Whitespace(
            1,
        ),
        38..39,
    ),
    (
        Text(
            "a",
        ),
        39..40,
    ),
    (
        Whitespace(
            1,
        ),
        40..41,
    ),
    (
        Text(
            "numbered",
        ),
        41..49,
    ),
    (
        Whitespace(
            1,
        ),
        49..50,
    ),
    (
        Text(
            "title",
        ),
        50..55,
    ),
    (
        Whitespace(
            1,
        ),
        55..56,
    ),
    (
        Text(
            "marker",
        ),
        56..62,
    ),
    (
        Period,
        62..63,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        63..64,
    ),
    (
        Dedent(
            [],
        ),
        0..0,
    ),
]
