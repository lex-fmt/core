---
source: tests/tokenizer_elements.rs
expression: tokens.tokens()
---
[
    (
        LexMarker,
        0..2,
    ),
    (
        Whitespace,
        2..3,
    ),
    (
        Text(
            "warning",
        ),
        3..10,
    ),
    (
        Whitespace,
        10..11,
    ),
    (
        Text(
            "severity",
        ),
        11..19,
    ),
    (
        Equals,
        19..20,
    ),
    (
        Text(
            "high",
        ),
        20..24,
    ),
    (
        Whitespace,
        24..25,
    ),
    (
        LexMarker,
        25..27,
    ),
    (
        Newline,
        27..28,
    ),
]
