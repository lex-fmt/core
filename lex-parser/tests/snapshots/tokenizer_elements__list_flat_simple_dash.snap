---
source: lex-parser/tests/tokenizer_elements.rs
expression: tokens
---
[
    (
        Text(
            "Test",
        ),
        0..4,
    ),
    (
        Colon,
        4..5,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        5..6,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        6..7,
    ),
    (
        Dash,
        7..8,
    ),
    (
        Whitespace(
            1,
        ),
        8..9,
    ),
    (
        Text(
            "First",
        ),
        9..14,
    ),
    (
        Whitespace(
            1,
        ),
        14..15,
    ),
    (
        Text(
            "item",
        ),
        15..19,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        19..20,
    ),
    (
        Dash,
        20..21,
    ),
    (
        Whitespace(
            1,
        ),
        21..22,
    ),
    (
        Text(
            "Second",
        ),
        22..28,
    ),
    (
        Whitespace(
            1,
        ),
        28..29,
    ),
    (
        Text(
            "item",
        ),
        29..33,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        33..34,
    ),
    (
        Dash,
        34..35,
    ),
    (
        Whitespace(
            1,
        ),
        35..36,
    ),
    (
        Text(
            "Third",
        ),
        36..41,
    ),
    (
        Whitespace(
            1,
        ),
        41..42,
    ),
    (
        Text(
            "item",
        ),
        42..46,
    ),
    (
        BlankLine(
            Some(
                "\n",
            ),
        ),
        46..47,
    ),
]
