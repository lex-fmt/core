//! Token definitions for the txxt format
//!
//! This module defines all the tokens that can be produced by the txxt lexer.
//! The tokens are defined using the logos derive macro for efficient tokenization.
//!
//! See docs/specs/<version>/grammar.txxt for the grammar of the txxt format.
use logos::Logos;
use std::fmt;

/// All possible tokens in the txxt format
#[derive(Logos, Debug, PartialEq, Eq, Hash, Clone, serde::Serialize, serde::Deserialize)]
pub enum Token {
    // Special markers
    #[token("::")]
    TxxtMarker,

    // Indentation (simplified - one token per 4 spaces or tab)
    #[regex(r" {4}|\t", priority = 3)] // Either 4 spaces OR 1 tab - highest priority
    Indent,

    // Semantic indentation tokens (generated by transformation)
    IndentLevel,
    DedentLevel,

    // Document boundary tokens (synthetic - added by lexer)
    // These solve the recursive/.repeated() EOF issue by providing explicit boundaries
    DocStart,
    DocEnd,

    // Whitespace (excluding newlines and indentation)
    #[regex(r" {1,3}", priority = 1)] // 1-3 spaces only, lower priority than indentation
    Whitespace,

    // Line breaks
    #[token("\n")]
    Newline,

    // Sequence markers
    #[token("-")]
    Dash,
    #[token(".")]
    Period,
    #[token("(")]
    OpenParen,
    #[token(")")]
    CloseParen,
    #[token(":")]
    Colon,

    // Parameter markers (for annotations)
    #[token(",")]
    Comma,
    #[token("\"")]
    Quote,
    #[token("=")]
    Equals,

    // Numbers (for ordered lists and session titles)
    #[regex(r"[0-9]+", |lex| lex.slice().to_owned(), priority = 2)]
    Number(String),

    // Text content (catch-all for non-special characters, excluding numbers and special chars)
    #[regex(r#"[^\s\n\t\-\.\(\):0-9,="']+"#, |lex| lex.slice().to_owned())]
    Text(String),
}

impl fmt::Display for Token {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let name = match self {
            Token::TxxtMarker => "txxt-marker",
            Token::Indent => "indent",
            Token::IndentLevel => "indent-level",
            Token::DedentLevel => "dedent-level",
            Token::Whitespace => "whitespace",
            Token::Newline => "newline",
            Token::Dash => "dash",
            Token::Period => "period",
            Token::OpenParen => "open-paren",
            Token::CloseParen => "close-paren",
            Token::Colon => "colon",
            Token::Comma => "comma",
            Token::Quote => "quote",
            Token::Equals => "equals",
            Token::Number(s) => return write!(f, "<number:{}>", s),
            Token::Text(s) => return write!(f, "<text:{}>", s),
            Token::DocStart => "doc-start",
            Token::DocEnd => "doc-end",
        };
        write!(f, "<{}>", name)
    }
}

impl Token {
    /// Check if this token represents indentation
    pub fn is_indent(&self) -> bool {
        matches!(self, Token::Indent)
    }

    /// Check if this token represents semantic indentation level
    pub fn is_indent_level(&self) -> bool {
        matches!(self, Token::IndentLevel)
    }

    /// Check if this token represents semantic dedentation level
    pub fn is_dedent_level(&self) -> bool {
        matches!(self, Token::DedentLevel)
    }

    /// Check if this token is whitespace (including indentation)
    pub fn is_whitespace(&self) -> bool {
        matches!(
            self,
            Token::Indent
                | Token::IndentLevel
                | Token::DedentLevel
                | Token::Whitespace
                | Token::Newline
        )
    }

    /// Check if this token is a sequence marker
    pub fn is_sequence_marker(&self) -> bool {
        matches!(
            self,
            Token::Dash | Token::Period | Token::OpenParen | Token::CloseParen
        )
    }

    /// Check if this token is a number
    pub fn is_number(&self) -> bool {
        matches!(self, Token::Number(_))
    }

    /// Check if this token is text content
    pub fn is_text(&self) -> bool {
        matches!(self, Token::Text(_))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::txxt_nano::lexer::tokenize;

    #[test]
    fn test_txxt_marker() {
        let tokens = tokenize("::");
        assert_eq!(tokens, vec![Token::TxxtMarker]);
    }

    #[test]
    fn test_indentation_tokens() {
        // Test 4 spaces
        let tokens = tokenize("    ");
        assert_eq!(tokens, vec![Token::Indent]);

        // Test tab
        let tokens = tokenize("\t");
        assert_eq!(tokens, vec![Token::Indent]);

        // Test multiple indent levels
        let tokens = tokenize("        "); // 8 spaces = 2 indent levels
        assert_eq!(tokens, vec![Token::Indent, Token::Indent]);
    }

    #[test]
    fn test_sequence_markers() {
        let tokens = tokenize("- . ( ) :");
        assert_eq!(
            tokens,
            vec![
                Token::Dash,
                Token::Whitespace,
                Token::Period,
                Token::Whitespace,
                Token::OpenParen,
                Token::Whitespace,
                Token::CloseParen,
                Token::Whitespace,
                Token::Colon
            ]
        );
    }

    #[test]
    fn test_text_tokens() {
        let tokens = tokenize("hello world");
        assert_eq!(
            tokens,
            vec![
                Token::Text("hello".to_string()),
                Token::Whitespace,
                Token::Text("world".to_string())
            ]
        );
    }

    #[test]
    fn test_newline_token() {
        let tokens = tokenize("\n");
        assert_eq!(tokens, vec![Token::Newline]);
    }

    #[test]
    fn test_mixed_content() {
        let tokens = tokenize("1. Hello world\n    - Item 1");
        assert_eq!(
            tokens,
            vec![
                Token::Number("1".to_string()),
                Token::Period,
                Token::Whitespace,
                Token::Text("Hello".to_string()),
                Token::Whitespace,
                Token::Text("world".to_string()),
                Token::Newline,
                Token::Indent,
                Token::Dash,
                Token::Whitespace,
                Token::Text("Item".to_string()),
                Token::Whitespace,
                Token::Number("1".to_string()),
            ]
        );
    }

    #[test]
    fn test_number_tokens() {
        let tokens = tokenize("123 456");
        assert_eq!(
            tokens,
            vec![
                Token::Number("123".to_string()),
                Token::Whitespace,
                Token::Number("456".to_string())
            ]
        );
    }

    #[test]
    fn test_token_predicates() {
        assert!(Token::Indent.is_indent());
        assert!(Token::IndentLevel.is_indent_level());
        assert!(Token::DedentLevel.is_dedent_level());
        assert!(!Token::Text("".to_string()).is_indent());

        assert!(Token::Indent.is_whitespace());
        assert!(Token::IndentLevel.is_whitespace());
        assert!(Token::DedentLevel.is_whitespace());
        assert!(Token::Whitespace.is_whitespace());
        assert!(Token::Newline.is_whitespace());
        assert!(!Token::Text("".to_string()).is_whitespace());

        assert!(Token::Dash.is_sequence_marker());
        assert!(Token::Period.is_sequence_marker());
        assert!(!Token::Text("".to_string()).is_sequence_marker());
        assert!(!Token::Number("".to_string()).is_sequence_marker());

        assert!(Token::Text("".to_string()).is_text());
        assert!(!Token::Dash.is_text());
        assert!(!Token::Number("".to_string()).is_text());

        assert!(Token::Number("".to_string()).is_number());
        assert!(!Token::Text("".to_string()).is_number());
        assert!(!Token::Dash.is_number());
    }
}
