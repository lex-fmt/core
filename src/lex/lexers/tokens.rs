//! Token definitions for the lex format
//!
//! This module defines all the tokens that can be produced by the lex lexer.
//! The tokens are defined using the logos derive macro for efficient tokenization.
//!
//! See docs/specs/`<version>`/grammar.lex for the grammar of the lex format.
use logos::Logos;
use std::fmt;

/// All possible tokens in the lex format
#[derive(Logos, Debug, PartialEq, Eq, Hash, Clone, serde::Serialize, serde::Deserialize)]
pub enum Token {
    // Special markers
    #[token("::")]
    LexMarker,

    // Indentation (simplified - one token per 4 spaces or tab)
    #[regex(r" {4}|\t", priority = 3)] // Either 4 spaces OR 1 tab - highest priority
    Indentation,

    // Semantic indentation tokens (generated by transformation)
    // These store the original tokens they were created from
    Indent(Vec<(Token, std::ops::Range<usize>)>),
    Dedent(Vec<(Token, std::ops::Range<usize>)>),

    // Semantic blank line token (synthetic - generated by transformation)
    // Replaces sequences of 2+ consecutive Newline tokens
    // Stores the original Newline tokens it replaced
    BlankLine(Vec<(Token, std::ops::Range<usize>)>),

    // Whitespace (excluding newlines and indentation)
    #[regex(r" {1,3}", priority = 1)] // 1-3 spaces only, lower priority than indentation
    Whitespace,

    // Line breaks
    #[token("\n")]
    Newline,

    // Sequence markers
    #[token("-")]
    Dash,
    #[token(".")]
    Period,
    #[token("(")]
    OpenParen,
    #[token(")")]
    CloseParen,
    #[token(":")]
    Colon,

    // Parameter markers (for annotations)
    #[token(",")]
    Comma,
    #[token("\"")]
    Quote,
    #[token("=")]
    Equals,

    // Numbers (for ordered lists and session titles)
    #[regex(r"[0-9]+", |lex| lex.slice().to_owned(), priority = 2)]
    Number(String),

    // Text content (catch-all for non-special characters, excluding numbers and special chars)
    #[regex(r#"[^\s\n\t\-\.\(\):0-9,=""]+"#, |lex| lex.slice().to_owned())]
    Text(String),
}

impl fmt::Display for Token {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        let name = match self {
            Token::LexMarker => "lex-marker",
            Token::Indentation => "indentation",
            Token::Indent(_) => "indent",
            Token::Dedent(_) => "dedent",
            Token::BlankLine(_) => "blank-line",
            Token::Whitespace => "whitespace",
            Token::Newline => "newline",
            Token::Dash => "dash",
            Token::Period => "period",
            Token::OpenParen => "open-paren",
            Token::CloseParen => "close-paren",
            Token::Colon => "colon",
            Token::Comma => "comma",
            Token::Quote => "quote",
            Token::Equals => "equals",
            Token::Number(s) => return write!(f, "<number:{}>", s),
            Token::Text(s) => return write!(f, "<text:{}>", s),
        };
        write!(f, "<{}>", name)
    }
}

impl Token {
    /// Check if this token represents indentation
    pub fn is_indent(&self) -> bool {
        matches!(self, Token::Indentation)
    }

    /// Check if this token represents semantic indentation level
    pub fn is_indent_level(&self) -> bool {
        matches!(self, Token::Indent(_))
    }

    /// Check if this token represents semantic dedentation level
    pub fn is_dedent_level(&self) -> bool {
        matches!(self, Token::Dedent(_))
    }

    /// Check if this token is whitespace (including indentation)
    pub fn is_whitespace(&self) -> bool {
        matches!(
            self,
            Token::Indentation
                | Token::Indent(_)
                | Token::Dedent(_)
                | Token::BlankLine(_)
                | Token::Whitespace
                | Token::Newline
        )
    }

    /// Check if this token is a sequence marker
    pub fn is_sequence_marker(&self) -> bool {
        matches!(
            self,
            Token::Dash | Token::Period | Token::OpenParen | Token::CloseParen
        )
    }

    /// Check if this token is a number
    pub fn is_number(&self) -> bool {
        matches!(self, Token::Number(_))
    }

    /// Check if this token is text content
    pub fn is_text(&self) -> bool {
        matches!(self, Token::Text(_))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::lex::lexers::tokenize;

    #[test]
    fn test_lex_marker() {
        let tokens: Vec<_> = tokenize("::").into_iter().map(|(t, _)| t).collect();
        assert_eq!(tokens, vec![Token::LexMarker]);
    }

    #[test]
    fn test_indentation_tokens() {
        // Test 4 spaces
        let tokens: Vec<_> = tokenize("    ").into_iter().map(|(t, _)| t).collect();
        assert_eq!(tokens, vec![Token::Indentation]);

        // Test tab
        let tokens: Vec<_> = tokenize("\t").into_iter().map(|(t, _)| t).collect();
        assert_eq!(tokens, vec![Token::Indentation]);

        // Test multiple indent levels
        let tokens: Vec<_> = tokenize("        ").into_iter().map(|(t, _)| t).collect(); // 8 spaces = 2 indent levels
        assert_eq!(tokens, vec![Token::Indentation, Token::Indentation]);
    }

    #[test]
    fn test_sequence_markers() {
        let tokens: Vec<_> = tokenize("- . ( ) :").into_iter().map(|(t, _)| t).collect();
        assert_eq!(
            tokens,
            vec![
                Token::Dash,
                Token::Whitespace,
                Token::Period,
                Token::Whitespace,
                Token::OpenParen,
                Token::Whitespace,
                Token::CloseParen,
                Token::Whitespace,
                Token::Colon
            ]
        );
    }

    #[test]
    fn test_text_tokens() {
        let tokens: Vec<_> = tokenize("hello world")
            .into_iter()
            .map(|(t, _)| t)
            .collect();
        assert_eq!(
            tokens,
            vec![
                Token::Text("hello".to_string()),
                Token::Whitespace,
                Token::Text("world".to_string())
            ]
        );
    }

    #[test]
    fn test_newline_token() {
        let tokens: Vec<_> = tokenize("\n").into_iter().map(|(t, _)| t).collect();
        assert_eq!(tokens, vec![Token::Newline]);
    }

    #[test]
    fn test_mixed_content() {
        let tokens: Vec<_> = tokenize("1. Hello world\n    - Item 1")
            .into_iter()
            .map(|(t, _)| t)
            .collect();
        assert_eq!(
            tokens,
            vec![
                Token::Number("1".to_string()),
                Token::Period,
                Token::Whitespace,
                Token::Text("Hello".to_string()),
                Token::Whitespace,
                Token::Text("world".to_string()),
                Token::Newline,
                Token::Indentation,
                Token::Dash,
                Token::Whitespace,
                Token::Text("Item".to_string()),
                Token::Whitespace,
                Token::Number("1".to_string()),
            ]
        );
    }

    #[test]
    fn test_number_tokens() {
        let tokens: Vec<_> = tokenize("123 456").into_iter().map(|(t, _)| t).collect();
        assert_eq!(
            tokens,
            vec![
                Token::Number("123".to_string()),
                Token::Whitespace,
                Token::Number("456".to_string())
            ]
        );
    }

    #[test]
    fn test_token_predicates() {
        assert!(Token::Indentation.is_indent());
        assert!(Token::Indent(vec![]).is_indent_level());
        assert!(Token::Dedent(vec![]).is_dedent_level());
        assert!(!Token::Text("".to_string()).is_indent());

        assert!(Token::Indentation.is_whitespace());
        assert!(Token::Indent(vec![]).is_whitespace());
        assert!(Token::Dedent(vec![]).is_whitespace());
        assert!(Token::BlankLine(vec![]).is_whitespace());
        assert!(Token::Whitespace.is_whitespace());
        assert!(Token::Newline.is_whitespace());
        assert!(!Token::Text("".to_string()).is_whitespace());

        assert!(Token::Dash.is_sequence_marker());
        assert!(Token::Period.is_sequence_marker());
        assert!(!Token::Text("".to_string()).is_sequence_marker());
        assert!(!Token::Number("".to_string()).is_sequence_marker());

        assert!(Token::Text("".to_string()).is_text());
        assert!(!Token::Dash.is_text());
        assert!(!Token::Number("".to_string()).is_text());

        assert!(Token::Number("".to_string()).is_number());
        assert!(!Token::Text("".to_string()).is_number());
        assert!(!Token::Dash.is_number());
    }
}
