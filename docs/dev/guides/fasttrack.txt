Lex Format Fasttrack


This is a high level overview of how the Lex parser is designed.

1. Processing Management

	The lex/pipeline manages a pipeline composed of lexing, parsing and building stages.

	This system allows us to:
		- keep different lexers and parser designs
		- offer a common interface for running these stages
		- easily access them by name, including in the lex binary
		- tests can easily load processed Lex inputs into a specific stage (or specific transform step) easily


2. Data Structures and General Design:

	1. Lexing
		1. The core lexing is done through the logos crate. These are the core tokens in tokens_core.rs
		2. Then a series of transformation steps are applied
		3. The pipeline converts logos tokens to a TokenStream (an enum that can be Flat or Tree)
		4. Most transformations output TokenStream::Flat, which can be unrolled to Vec<(Token, Range<usize>)>
		5. The final lexer output is typically TokenStream::Flat

	2. Parsing: Semantic Analysis
		1. The parsing stage differentiates between actual analysis and AST building
		2. lex/parsing does semantic analysis and returns an intermediate representation (IR)
		3. The IR is a tree where each node specifies which AST node to create and which tokens to use
		4. Parsing takes a flat token vector and outputs IR Nodes

	3. Building
		1. Final stage in the process
		2. Receives the IR nodes and walks the tree, creating actual AST nodes with the tokens given
		3. During this process, node span ranges are translated into AST Location objects


3. Parser Design:

	The project now runs exclusively on the linebased parser. The multi-parser plumbing (AnalyzerConfig, pipeline registry, etc.) remains so we can stand up alternative analyzers in the future, but the active implementation is the line-based grammar engine.

	Linebased parser overview:

		- Receives the same flat Vec<(Token, Range<usize>)> produced by the lexing pipeline
		- Groups tokens into logical lines and indentation scopes (tree_builder module)
		- Classifies each line (SubjectLine, ListLine, etc.) and wraps them in LineContainer nodes
		- Uses declarative regex patterns to match container sequences and emit IR nodes

	The grammar at the container level is simple enough to be expressed as ordered regular expressions, which is what the declarative_grammar engine does. Tree building complexity is isolated inside the parser, not in the pipeline.

	Key files:
		- src/lex/parsing/linebased/tree_builder.rs (tree construction)
		- src/lex/parsing/linebased/declarative_grammar.rs (pattern matching)
		- src/lex/lexing/linebased/tokens_linebased.rs (LineToken/LineContainer types)
