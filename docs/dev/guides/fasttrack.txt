Lex Format Fasttrack


This is a high level overview of how the Lex parser is designed.

1. Processing Management

	The lex/pipeline manages a pipeline composed of lexing, parsing and building stages.

	This system allows us to:
		- keep different lexers and parser designs
		- offer a common interface for running these stages
		- easily access them by name, including in the lex binary
		- tests can easily load processed Lex inputs into a specific stage (or specific transform step) easily


2. Data Structures and General Design:

	1. Lexing
		1. The core lexing is done through the logos crate. These are the core tokens in tokens_core.rs
		2. Then a series of transformation steps are applied
		3. The pipeline converts logos tokens to a TokenStream (now just a type alias for Vec<(Token, Range<usize>)>)
		4. Transformations are chained: each receives a flat vector and returns a flat vector
		5. The final lexer output is a flat Vec<(Token, Range<usize>)>

	2. Parsing: Semantic Analysis
		1. The parsing stage differentiates between actual analysis and AST building
		2. lex/parsing does semantic analysis and returns an intermediate representation (IR)
		3. The IR is a tree where each node specifies which AST node to create and which tokens to use
		4. Parsing takes a flat token vector and outputs IR Nodes

	3. Building
		1. Final stage in the process
		2. Receives the IR nodes and walks the tree, creating actual AST nodes with the tokens given
		3. During this process, node span ranges are translated into AST Location objects


3. Parser Designs:

	We currently have two parser designs active.

	1. Reference

		This is the initial parser. It uses the chumsky parser combinator library for analysis but offloads actual AST building to the building stage.

		It operates on the core tokens with three transformations:
			- NormalizeWhitespace
			- SemanticIndentation (converts Indentation to Indent/Dedent tokens)
			- BlankLines

		Receives: flat Vec<(Token, Range<usize>)>
		Uses directly for combinator parsing

	2. Linebased

		This parser leverages Lex's specific constraints that make parsing simple despite appearing complex (recursive, stateful, indentation-significant).

		Receives: flat Vec<(Token, Range<usize>)> from same pipeline as reference parser

		Internal tree building:
			- Groups tokens into lines (tree_builder module)
			- Classifies lines (SubjectLine, ListLine, etc.)
			- Groups into hierarchical levels (indent/dedent)
			- Creates LineContainer tree structure

		The grammar at the container level is so simple it can be expressed as regular expressions. That's what the declarative_grammar engine does.

		Tree building complexity is localized inside the linebased parser, not in the pipeline.

		Key files:
			- src/lex/parsing/linebased/tree_builder.rs (tree construction)
			- src/lex/parsing/linebased/declarative_grammar.rs (pattern matching)
			- src/lex/lexing/linebased/tokens_linebased.rs (LineToken/LineContainer types)


4. Pipeline Simplification (Recent Refactoring):

	TokenStream was simplified from a complex enum (Flat/Tree variants) to a simple type alias:

		pub type TokenStream = Vec<(Token, Range<usize>)>

	Benefits:
		- Pipeline stays flat and simple
		- Tree building moved into linebased parser (only consumer that needs it)
		- Removed ~1,100 lines of tree-walking infrastructure
		- Better locality of complexity
		- Easier to understand and maintain

	The transformation pipeline now purely handles flat token transformations.
	Parsers consume flat vectors and build any needed structure internally.
